version: '3.8'

services:
  # ============================================================================
  # Infrastructure Services
  # ============================================================================

  postgres:
    image: postgres:15-alpine
    container_name: nodus-adk-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-nodus}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-nodus_dev_password}
      POSTGRES_DB: ${POSTGRES_DB:-nodus}
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./config/postgres:/docker-entrypoint-initdb.d
    networks:
      - nodus-adk-internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-nodus}"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: nodus-adk-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis:/data
    networks:
      - nodus-adk-internal
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  qdrant:
    image: qdrant/qdrant:latest
    container_name: nodus-adk-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./data/qdrant:/qdrant/storage
    networks:
      - nodus-adk-internal
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  minio:
    image: minio/minio:latest
    container_name: nodus-adk-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio:/data
    networks:
      - nodus-adk-internal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # ============================================================================
  # Observability: Langfuse (LLM Tracing & Observability)
  # ============================================================================

  langfuse:
    image: langfuse/langfuse:2
    container_name: nodus-adk-langfuse
    restart: unless-stopped
    environment:
      # PostgreSQL connection
      DATABASE_URL: postgresql://${POSTGRES_USER:-nodus}:${POSTGRES_PASSWORD:-nodus_dev_password}@postgres:5432/langfuse_db
      
      # NextAuth configuration
      NEXTAUTH_SECRET: ${LANGFUSE_SECRET_KEY:-langfuse_dev_secret_change_in_production}
      SALT: ${LANGFUSE_SECRET_KEY:-langfuse_dev_secret_change_in_production}
      NEXTAUTH_URL: http://localhost:3000
      
      # Disable telemetry
      TELEMETRY_ENABLED: false
      
      # Enable experimental features (prompt management, etc.)
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: true
      
      # Server configuration
      HOSTNAME: 0.0.0.0
      PORT: 3000
      
    ports:
      - "3000:3000"
    
    volumes:
      - ./data/langfuse:/app/uploads
    
    networks:
      - nodus-adk-internal
      - nodus-adk-edge
    
    depends_on:
      postgres:
        condition: service_healthy
    
    healthcheck:
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:3000/api/public/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1));\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "3"

  # ============================================================================
  # OpenMemory (Memory Layer - DISABLED, using direct Qdrant instead)
  # Reason: JWT expiration in batch, no vector generation, complex setup
  # ============================================================================

  # DISABLED - OpenMemory replaced by direct Qdrant access
  # Reasons: JWT expiration in batch, no vector generation, overly complex
  # New approach: Direct Qdrant with separate collections for memory vs knowledge
  
  # openmemory:
  #   build:
  #     context: /Users/quirze/Factory/nodus-os/repos/nodus-memory/backend
  #     dockerfile: Dockerfile
  #   container_name: nodus-adk-openmemory
  #   ...

  # openmemory-dashboard:
  #   build:
  #     context: /Users/quirze/Factory/nodus-os/repos/nodus-memory/dashboard
  #     dockerfile: Dockerfile.prod
  #   container_name: nodus-adk-openmemory-dashboard
  #   ...

  # ============================================================================
  # Nodus OS Services (from cloned repos)
  # ============================================================================

  backoffice:
    build:
      context: ../nodus-backoffice
      dockerfile: Dockerfile
    container_name: nodus-adk-backoffice
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-nodus}:${POSTGRES_PASSWORD:-nodus_dev_password}@postgres:5432/${POSTGRES_DB:-nodus}
      REDIS_URL: redis://redis:6379/0
      QDRANT_URL: http://qdrant:6333
      QDRANT_API_KEY: ${QDRANT_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      NODE_ENV: development
      PORT: 5001
      JWT_JWKS_URL: http://localhost:5001/.well-known/jwks.json
      JWT_ISSUER: backoffice
      JWT_AUDIENCE: backoffice
      # Google Credentials for Token Generation
      GOOGLE_WORKSPACE_SA_CLIENT_EMAIL: ${GOOGLE_WORKSPACE_SA_CLIENT_EMAIL}
      GOOGLE_WORKSPACE_SA_PRIVATE_KEY: ${GOOGLE_WORKSPACE_SA_PRIVATE_KEY}
      # Google OAuth2 for SSO Login
      GOOGLE_OAUTH_CLIENT_ID: ${GOOGLE_OAUTH_CLIENT_ID:-}
      GOOGLE_OAUTH_CLIENT_SECRET: ${GOOGLE_OAUTH_CLIENT_SECRET:-}
      GOOGLE_OAUTH_REDIRECT_URI: ${GOOGLE_OAUTH_REDIRECT_URI:-http://localhost:5001/auth/google/callback}
      BACKOFFICE_PUBLIC_URL: ${BACKOFFICE_PUBLIC_URL:-http://localhost:5001}
      BACKOFFICE_SERVICE_TOKEN: ${BACKOFFICE_SERVICE_TOKEN}
      # Database migrations control
      RUN_MIGRATIONS: ${RUN_MIGRATIONS:-true}
    ports:
      - "5001:5001"
    volumes:
      - ../nodus-backoffice:/app
      - /app/node_modules
      - ./data/backoffice-uploads:/app/storage/uploads
    networks:
      - nodus-adk-internal
      - nodus-adk-edge
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    user: root
    entrypoint: ["/app/docker-entrypoint.sh"]

  llibreta:
    build:
      context: ../nodus-llibreta
      dockerfile: Dockerfile
    container_name: nodus-adk-llibreta
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-nodus}:${POSTGRES_PASSWORD:-nodus_dev_password}@postgres:5432/${POSTGRES_DB:-nodus}
      AUTH_DATABASE_URL: postgresql://${POSTGRES_USER:-nodus}:${POSTGRES_PASSWORD:-nodus_dev_password}@postgres:5432/${POSTGRES_DB:-nodus}
      REDIS_URL: redis://redis:6379/0
      BACKOFFICE_URL: http://backoffice:5001
      ADK_RUNTIME_URL: http://adk-runtime:8080
      PERSONAL_ASSISTANT_ENGINE: ${PERSONAL_ASSISTANT_ENGINE:-graph}
      NODE_ENV: development
      PORT: 5002
    ports:
      - "5002:5002"
    volumes:
      - ../nodus-llibreta:/app
      - llibreta_node_modules:/app/node_modules
    networks:
      - nodus-adk-internal
      - nodus-adk-edge
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      backoffice:
        condition: service_started
    user: "0:0"
    command: sh -c "chown -R 1000:1000 /app/node_modules 2>/dev/null || true && chown -R 1000:1000 /app 2>/dev/null || true && su node -c 'cd /app && npm install --include=dev && npx concurrently -n backend,frontend -c bgBlue.bold,bgMagenta.bold \"npm run dev:simple\" \"vite\"'"

  mcp-gateway:
    build:
      context: ../nodus-mcp-gateway
      dockerfile: Dockerfile
    container_name: nodus-adk-mcp-gateway
    environment:
      PORT: 7443
      REDIS_URL: redis://redis:6379/1
      BACKOFFICE_URL: http://backoffice:5001
      LOG_LEVEL: debug
      BACKOFFICE_SERVICE_TOKEN: ${BACKOFFICE_SERVICE_TOKEN}
    ports:
      - "7443:7443"
    volumes:
      - ../nodus-mcp-gateway:/app
      - /app/node_modules
    networks:
      - nodus-adk-internal
    depends_on:
      redis:
        condition: service_healthy
      backoffice:
        condition: service_started
    command: sh -c "npm install --include=dev && npm run dev"

  # B2BRouter MCP Server (local deployment for development)
  #mcp-b2brouter:
  #  build:
  #    context: ../nodus-mcp-b2brouter
  #    dockerfile: Dockerfile
  #  container_name: nodus-adk-mcp-b2brouter
  #  environment:
  #    PORT: 3001
  #    B2BROUTER_API_KEY: ${B2BROUTER_API_KEY:-3194b66154a8f6fda9ebaa50c35d385ee354df23}
  #    B2BROUTER_ENVIRONMENT: ${B2BROUTER_ENVIRONMENT:-staging}
  #    NODE_ENV: development
  #  ports:
  #    - "3001:3001"
  #  volumes:
  #    - ../nodus-mcp-b2brouter:/app
  #    - /app/node_modules
  #  networks:
  #    - nodus-adk-internal
  #    - nodus-adk-edge
  #  command: sh -c "npm install && npm run build && node dist/http-server.js"

  # ============================================================================
  # Additional Services (Commented - Enable when needed)
  # ============================================================================

  # Infisical Secrets Manager
  # Note: Database initialization is handled automatically by 03-create-infisical.sh
  # during PostgreSQL's first initialization (when volume is new)
  infisical:
    image: infisical/infisical:v0.149.0
    container_name: nodus-adk-infisical
    restart: unless-stopped
    environment:
      ENCRYPTION_KEY: "${INFISICAL_ENCRYPTION_KEY}"
      AUTH_SECRET: "${INFISICAL_AUTH_SECRET}"
      DB_CONNECTION_URI: "postgresql://${INFISICAL_DB_USER:-infisical_service}:${INFISICAL_DB_PASSWORD:-change-me-infisical}@postgres:5432/${INFISICAL_DB_NAME:-infisical_compat149}"
      REDIS_URL: "redis://redis:6379/2"
      SITE_URL: "http://localhost:8081"
      HOST: "0.0.0.0"
      PORT: "8080"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "127.0.0.1:8081:8080"
    networks:
      - nodus-adk-internal

  # Avatar Proxy Service
  #avatar-proxy:
  #  build:
  #    context: ../nodus-backoffice/avatar/proxy
  #  container_name: nodus-adk-avatar-proxy
  #  restart: unless-stopped
  #  environment:
  #    NODE_ENV: ${NODE_ENV:-production}
  #  networks:
  #    - nodus-adk-internal
  #    - nodus-adk-edge
  #  env_file:
  #    - ./.env
  #  depends_on:
  #    - backoffice

  # Avatar Widget Service
  #avatar-widget:
  #  build:
  #    context: ../nodus-backoffice/avatar/widget
  #  container_name: nodus-adk-avatar-widget
  #  restart: unless-stopped
  #  environment:
  #    NODE_ENV: ${NODE_ENV:-production}
  #    WIDGET_PUBLIC_URL: https://${SERVER_NAME}/avatar
  #    BACKEND_CHAT_URL: https://${SERVER_NAME}/api
  #    HEYGEN_TOKEN_URL: https://${SERVER_NAME}/api/heygen/token
  #  networks:
  #    - nodus-adk-internal
  #    - nodus-adk-edge
  #  env_file:
  #    - ./.env
  #  depends_on:
  #    - avatar-proxy

  # Nginx Reverse Proxy
  #nginx:
  #  image: nginx:alpine
  #  container_name: nodus-adk-nginx
  #  restart: unless-stopped
  #  ports:
  #    - "80:80"
  #  volumes:
  #    - ../nodus-backoffice/nginx.conf:/etc/nginx/nginx.conf:ro
  #  networks:
  #    - nodus-adk-internal
  #    - nodus-adk-edge
  #  depends_on:
  #    - backoffice
  #    - avatar-widget
  #    - avatar-proxy
  #    - mcp-gateway
  #  healthcheck:
  #    test: ["CMD", "nginx", "-t"]
  #    interval: 30s
  #    timeout: 10s
  #    retries: 3
  #    start_period: 30s

  # OCR Service
  #ocr-service:
  #  build:
  #    context: ../nodus-backoffice/services/ocr
  #    dockerfile: Dockerfile
  #  container_name: nodus-adk-ocr-service
  #  restart: unless-stopped
  #  environment:
  #    OCR_TOKEN: ${OCR_TOKEN:-supersecrettoken}
  #    OCR_LANG: ${OCR_LANG:-cat,spa,eng}
  #  networks:
  #    - nodus-adk-internal
  #  env_file:
  #    - ./.env

  # ============================================================================
  # AI Gateway (LiteLLM Self-Hosted)
  # ============================================================================
  litellm:
    image: ghcr.io/berriai/litellm:main-v1.52.11  # Debian-based, supports libsndfile. Note: No modern dashboard, only Swagger UI
    container_name: nodus-adk-litellm
    ports:
      - "4000:4000"
    volumes:
      - ./config/litellm:/app/config
    command: [ "--config", "/app/config/litellm_config.yaml", "--port", "4000", "--detailed_debug" ]
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-nodus}:${POSTGRES_PASSWORD:-nodus_dev_password}@postgres:5432/litellm_db
      - STORE_MODEL_IN_DB=True
      
      # Provider API Keys
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GEMINI_API_KEY=${GOOGLE_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      
      # Master Key (used for API auth and Admin UI login)
      # Admin UI: username=admin, password=LITELLM_MASTER_KEY
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-nodus-master-key}
      
      # Langfuse integration for observability
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY:-pk-lf-a401fb0c-6ee3-4636-afd4-803b9dfe4aaf}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY:-sk-lf-ccb62e83-9148-49f8-8858-ff3c963bb7a8}
      - LANGFUSE_HOST=http://langfuse:3000
    networks:
      - nodus-adk-internal
    depends_on:
      - postgres
      - redis
      - langfuse

  # ============================================================================
  # ADK Services (from new repos)
  # ============================================================================

  agent-pool:
    build:
      context: ../nodus-adk-agents
      dockerfile: Dockerfile.pool
    container_name: nodus-adk-agent-pool
    environment:
      LOG_LEVEL: INFO
      PORT: 8000
      
      # Langfuse observability for A2A agents
      LANGFUSE_ENABLED: ${LANGFUSE_ENABLED:-true}
      LANGFUSE_HOST: http://langfuse:3000
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-pk-lf-a401fb0c-6ee3-4636-afd4-803b9dfe4aaf}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-sk-lf-ccb62e83-9148-49f8-8858-ff3c963bb7a8}
      
      # OpenTelemetry configuration
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://langfuse:3000/api/public/ingestion}
      OTEL_SERVICE_NAME: ${OTEL_SERVICE_NAME_A2A:-nodus-adk-agent-pool}
      OTEL_TRACES_SAMPLER: ${OTEL_TRACES_SAMPLER:-parentbased_traceidratio}
      OTEL_TRACES_SAMPLER_ARG: ${OTEL_TRACES_SAMPLER_ARG:-1.0}
    ports:
      - "8000:8000"
    networks:
      - nodus-adk-internal
    depends_on:
      langfuse:
        condition: service_healthy

  adk-runtime:
    build:
      context: ../nodus-adk-runtime
      dockerfile: Dockerfile.dev
    container_name: nodus-adk-runtime
    environment:
      # Database & Services
      DATABASE_URL: postgresql://${POSTGRES_USER:-nodus}:${POSTGRES_PASSWORD:-nodus_dev_password}@postgres:5432/${POSTGRES_DB:-nodus}
      REDIS_URL: redis://redis:6379/0
      BACKOFFICE_URL: http://backoffice:5001
      MCP_GATEWAY_URL: http://mcp-gateway:7443
      QDRANT_URL: http://qdrant:6333
      QDRANT_API_KEY: ${QDRANT_API_KEY:-}
      
      # AI Models (via LiteLLM proxy)
      # Using Gemini model recognized by ADK, but LiteLLM routes it
      ADK_MODEL: ${ADK_MODEL:-gpt-4o}
      ADK_PROJECT_ID: ${ADK_PROJECT_ID:-}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY:-}  # Not used - routed via LiteLLM
      
      # Server Config
      LOG_LEVEL: ${LOG_LEVEL:-DEBUG}
      PORT: 8080
      CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost:5002,http://localhost:5001,http://localhost:3000}
      
      # Langfuse Observability
      LANGFUSE_ENABLED: ${LANGFUSE_ENABLED:-true}
      LANGFUSE_HOST: http://langfuse:3000
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-pk-lf-a401fb0c-6ee3-4636-afd4-803b9dfe4aaf}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-sk-lf-ccb62e83-9148-49f8-8858-ff3c963bb7a8}
      LANGFUSE_SDK_INTEGRATION: nodus-adk
      
      # LiteLLM Proxy Configuration (unified AI gateway for ALL models)
      # ALL LLM calls (OpenAI, Gemini, Claude) go through this proxy
      OPENAI_API_BASE: http://litellm:4000/v1
      OPENAI_API_KEY: ${LITELLM_MASTER_KEY:-sk-nodus-master-key}
      LITELLM_PROXY_API_BASE: http://litellm:4000
      LITELLM_PROXY_API_KEY: ${LITELLM_MASTER_KEY:-sk-nodus-master-key}
      
      # OpenTelemetry Configuration
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://langfuse:3000/api/public/ingestion}
      OTEL_SERVICE_NAME: ${OTEL_SERVICE_NAME:-nodus-adk-runtime}
      OTEL_TRACES_SAMPLER: ${OTEL_TRACES_SAMPLER:-parentbased_traceidratio}
      OTEL_TRACES_SAMPLER_ARG: ${OTEL_TRACES_SAMPLER_ARG:-1.0}
      
      # ADK Telemetry
      ADK_CAPTURE_MESSAGE_CONTENT_IN_SPANS: ${ADK_CAPTURE_MESSAGE_CONTENT_IN_SPANS:-true}
      
    ports:
      - "8080:8080"
    volumes:
      - ../nodus-adk-runtime/src:/app/src
      - ../nodus-adk-agents:/nodus-adk-agents
    networks:
      - nodus-adk-internal
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      backoffice:
        condition: service_started
      mcp-gateway:
        condition: service_started
      langfuse:
        condition: service_healthy

# ============================================================================
# Networks
# ============================================================================

networks:
  nodus-adk-internal:
    name: nodus-adk-internal
    driver: bridge
  nodus-adk-edge:
    name: nodus-adk-edge
    driver: bridge

# ============================================================================
# Volumes
# ============================================================================
# Note: Critical data volumes (postgres, redis, qdrant, minio, langfuse) use
# host bind mounts at ./data/* to ensure:
# 1. Data persists across container rebuilds/removals
# 2. Data is backed up by Time Machine (macOS) automatically
# 3. Data can be version controlled (if desired) or manually backed up
# 4. Compatible with Hetzner staging/prod backups

volumes:
  # Ephemeral volumes only
  llibreta_node_modules:
    name: nodus-adk-llibreta-node-modules
